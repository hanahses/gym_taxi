{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c00af1-78e1-488d-82bc-0d54cc755bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c55161-774d-4b7b-a083-e640bc199e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c8aa58-667a-4ad5-b16f-5b5dc6fbdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametros do agente\n",
    "\n",
    "alpha = 0.9 # É a taxa de aprendizado, define o quanto novas informações sobrepõe as antigas\n",
    "gamma = 0.95 # O quão importante são as recompensas\n",
    "epsilon = 1 # Aleatoriedade das ações\n",
    "epsilon_decay = 0.995 # Queda do epsilon\n",
    "min_epsilon = 0.01 #Número mínimo do epsilon, garante sempre uma mínima aleatóriedade\n",
    "num_episodes = 10000\n",
    "max_steps = 100 # Quantidade maxima de ações que o agente pode realizar até um episódio ser encerrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07804b65-b17c-45f6-bf4a-c065824e5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-table\n",
    "\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n)) # quantos estados o jogo pode ter e quantas ações posso tomar por estado \n",
    "\n",
    "# ****analisar implementação\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e7fc00-cc58-41b8-8db2-787e292d33ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3246538186.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    if random.uniform(a:0, b:1) < epsilon:\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#funcionamento das escolhas das ações\n",
    "\n",
    "def choose_action(state): #verifica as ações onde esse estado é relevante e retorna a que tem o maior q_value\n",
    "    if random.uniform(a:0, b:1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(q_table[state, :]\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "        state, _ = env.reset() #escolhendo aleatoriamente um ponto de inicio\n",
    "\n",
    "        done = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = choose_action(state)\n",
    "\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            old_value = q_table[state,action] #caso tome uma ação que já tenha um valor presente na q_table\n",
    "            next_max = np.max(q_table[next_state, :]) #toma a ação, verifica qual ação seguinte tem o maior q_value e atualiza com o valor da maior recompensa\n",
    "\n",
    "            q_table[state, action] = (1- alpha) * old_value + alpha * (reward + gamma *next_max) #equação de bellman\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay) #reduz caso não esteja abaixo do mínimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5fbf2-484a-4a17-83eb-95f60391f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iniciando o ambiente (NÃO CONSIGO RODAAAAAAAAAAAR)\n",
    "\n",
    "env = gym.make(id:'Taxy-V3', render_mode='human')\n",
    "\n",
    "for episode in range(5):\n",
    "    state, _ - env.reset()\n",
    "    done = False\n",
    "\n",
    "    print('Episode', episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        env.render()\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        if done or truncated:\n",
    "            env.render()\n",
    "            print('Finished episode', episode, 'with reward', reward)\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
